# JokeyJokes
Using Machine Learning to generate joke concepts

1) Scrape Clickhole (or any website) for content to train Machine Learning algorith on
2) Clean up text generated from scraping
3) Publish training data text on a publicly accessible website (such as Github...)
4) Use existing Google collab to train against the data, and generate new jokes
_______________________

## 1) Scrape Clickhole (or any website) for content to train Machine Learning algorith on.

I used a python tool called "Scrapy" https://scrapy.org/ 
Should be able to install with `pip install scrapy`, or `python3 -m pip install Scrapy` or similar.  See website or stack overflow for installation help if having issues.

### Long Install:

Navigate terminal to a new directory you want to work in, and run `scrapy startproject tutorial`

Navigate terminal to `tutorial` sub-directory, where a `scrapy.cfg` file should have been created.

Create a .py file in the `tutorial/spiders/` directory called `quotes_spider.py`.  Populate that py file with the content in the `tutorial/spiders/quotes_spider.py` file in this repo:
```
import scrapy

# to run, nav terminal to directory with scrapy.cfg file, and run `scrapy crawl clickhole -o FullArticles.json`
class ClickholeSpider(scrapy.Spider):
    name = "clickhole"
    start_urls = [
        'https://clickhole.com/',
    ]

    def parse(self, response):

        yield { 
            'headline': response.css("h2.post-title a::text").getall(),
            # 'article': response.css("div.post-content p span::text").getall(),
            }

        # for article in response.css("h2.post-title a::attr(href)"):
        #     yield scrapy.Request(article.get(), callback=self.parse)

        next_page = response.css("a.next::attr(href)").get()
        if next_page is not None:
            yield scrapy.Request(next_page, callback=self.parse)
```

### Short Install: just use the files provided in this repo.  Still need to run `pip install scrapy`, or `python3 -m pip install Scrapy` or similar though.

To run the scraper, navigate terminal to the directory with the `scrapy.cfg` file in it, and run `scrapy crawl clickhole -o Headlines.json`.  This should generate a huge list of every article headline in the entire Clickhole website, and save it to a file called `Headlines.json` in the same directory as the terminal.

To collect full text for every article as well, un-comment the commented our lines in the `quotes_spider.py` file:
```
import scrapy

# to run, nav terminal to directory with scrapy.cfg file, and run `scrapy crawl clickhole -o FullArticles.json`
class ClickholeSpider(scrapy.Spider):
    name = "clickhole"
    start_urls = [
        'https://clickhole.com/',
    ]

    def parse(self, response):

        yield { 
            'headline': response.css("h2.post-title a::text").getall(),
            'article': response.css("div.post-content p span::text").getall(),
            }

        for article in response.css("h2.post-title a::attr(href)"):
            yield scrapy.Request(article.get(), callback=self.parse)

        next_page = response.css("a.next::attr(href)").get()
        if next_page is not None:
            yield scrapy.Request(next_page, callback=self.parse)
```

To collect only text for a single article tag (e.g. "wow" or "so sad", or any of the tags used by Clickhole), change the `start_urls` to 

`https://clickhole.com/tag/wow/`
or 
`https://clickhole.com/tag/probably-illegal/`
or whatever tag you'd like.

## 2) Clean Up Generated Text

The text generated by scraping off the html will be a little messy, with artifacts that you don't want messing up a machine learning training data set.  Just copy / paste the text data into your favorite text editor (I'm a fan of Visual Studio Code), and find / replace until the text data is looking good and ready to be trained against.

## 3) Publish Cleaned Text Data on Publicly-Accessible Platform

GitHub makes this easy.  You can even just add the raw .txt file to a repo by copy / pasting in a new file through the browser if you'd like.

## 4) Use existing Google collab to train against the data, and generate new jokes

Here's a helpful google collab that takes care of all the hard stuff: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb

It's a tutorial which by default trains off of a bunch of text from Shakespear plays, and uses it to auto-generate new "Shakespearean" content.

In order to modify it for our purposes, change the following line:
`path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')`
to 
`path_to_file = tf.keras.utils.get_file('MyAwesomeTrainingText.txt', 'https://raw.githubusercontent.com/Pierce-Evan/JokeyJokes/main/MyAwesomeTrainingText.txt')`
Or, whatever you named your training text file when you published it in step 3.  Just make sure the `MyAwesomeTrainingText.txt` name matches the file in the full url.  If you want to use a ready-made example, you can use
`path_to_file = tf.keras.utils.get_file('Articles2.txt', 'https://raw.githubusercontent.com/Pierce-Evan/Test/main/Articles2.txt')`

That's it!  Then, just let the Google Collab tutorial do all the heavy lifting.  Walk through from top to bottom, executing every code block until it auto-generates content in the "Generate Text" stage.

There are lots of ways to customize and tinker with the algorithm, but this is enough to get you started.

## Notes
The generated content is a little "Jabberwocky" currently.  This could be due to an insufficient training data size.  I haven't yet pulled in content from The Onion or elsewhere to give it more data to train with.  Also, different articles use different "voices", which makes it harder for a machine-learning amalgamation of every article to be coherent and optimized.  So it could be that filtering the training data may bear fruit.  There are also many ways to modify the machine learning algorithm which could improve results.  

There are also some articles like the following which surely don't help in the "Jabberwocky" department: https://clickhole.com/und-hungry-hogg-have-aten-oll-th-dvds-een-aur-vellage-laster-nighten-kann-you-reminder-us-ov-wart-happen-en-theese-dvds-plase-goodest-educated-sir/. Feel free to manually remove any such content from the training data if you'd like.
